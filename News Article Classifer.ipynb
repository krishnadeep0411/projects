{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Binary Classifiers - Machine Learning and Foundations - Final Assignment**"
      ],
      "metadata": {
        "id": "tVT6wT_aWIcS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Importing required libraries***"
      ],
      "metadata": {
        "id": "AnlChW_AWSzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "RF0oasT68lZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrJ0W2JP2gYf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.manifold import TSNE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Setting SEED to Ensure Reproducibility***"
      ],
      "metadata": {
        "id": "62a2HsbZWabB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seed and downloads\n",
        "SEED = 42\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ],
      "metadata": {
        "id": "gM6cpWRZR4Ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/ML Final/2.csv')\n",
        "df = df.drop(columns=['Unnamed: 0', 'authors', 'link', 'date'])\n",
        "print(df.head())\n",
        "df['combined_text'] = df['headline'] + \" \" + df['short_description']\n",
        "df.drop(columns=['headline', 'short_description'], inplace=True)\n",
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "Brj08fWaWvae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropped authors, link and date as they won't be relevant to extract features from."
      ],
      "metadata": {
        "id": "U0I2HKCUX4sD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to clean the text data and prepare it for analysis."
      ],
      "metadata": {
        "id": "l_wdyF7UYD89"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBHDiJadOxx2"
      },
      "outputs": [],
      "source": [
        "# Clean text\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z?.!,Â¿]+|http\\S+\", \" \", text)\n",
        "    text = ''.join([char for char in text if char not in string.punctuation])\n",
        "    words = [word for word in text.split() if word not in stop_words]\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return \" \".join(words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['cleaned_text'] = df['combined_text'].apply(clean_text)"
      ],
      "metadata": {
        "id": "UpL5WxycpZo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The category distribution below shows an imbalance in our dataset where more articles are in the 'ENTERTAINMENT' category than in 'STYLE.' This means that the STYLE category is underrepresented in our dataset, this might lead to errors in classifying the articles from that category."
      ],
      "metadata": {
        "id": "XkDdKG56YNgt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMiT9MpxK_5N"
      },
      "outputs": [],
      "source": [
        "# Plot category distribution\n",
        "def plot_category_distribution(data):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.countplot(data=data, x='category')\n",
        "    plt.title('Number of Articles in Each Category')\n",
        "    plt.xlabel('Category')\n",
        "    plt.ylabel('Number of Articles')\n",
        "    plt.show()\n",
        "\n",
        "plot_category_distribution(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the most frequent words from both categories."
      ],
      "metadata": {
        "id": "q1oJCHdKYwnS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiYxpKtWRFv_"
      },
      "outputs": [],
      "source": [
        "# Plot most common words for each category\n",
        "def plot_most_common_words(data, category, num_words=20):\n",
        "    category_data = data[data['category'] == category]\n",
        "    all_words = [word for text in category_data['cleaned_text'] for word in text.split()]\n",
        "    word_freq = Counter(all_words).most_common(num_words)\n",
        "    words, counts = zip(*word_freq)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=counts, y=words, hue=words, palette='viridis')\n",
        "    plt.title(f'Most Common Words in {category}')\n",
        "    plt.xlabel('Frequency')\n",
        "    plt.ylabel('Words')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Cfc_H8pRHw8"
      },
      "outputs": [],
      "source": [
        "categories = df['category'].unique()\n",
        "for category in categories:\n",
        "    plot_most_common_words(df, category)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mapping the categorical variables to numbers for training purposes."
      ],
      "metadata": {
        "id": "rRZE_vQcY3GL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hjfnn-CSRL8K"
      },
      "outputs": [],
      "source": [
        "# Prepare data for training\n",
        "df_cleaned = df[['category', 'cleaned_text']].dropna()\n",
        "category_mapping = {'ENTERTAINMENT': 0, 'STYLE': 1}\n",
        "df_cleaned['category'] = df_cleaned['category'].map(category_mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Splitting the Data***:\n",
        "Our original dataset has been split into train, valid and test datasets in the ratio 70:15:15."
      ],
      "metadata": {
        "id": "C7lVtI2XY-Y8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJ-9WkIE4GME"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = train_test_split(df_cleaned, test_size=0.3, random_state=SEED, stratify=df_cleaned['category'])\n",
        "valid_data, test_data = train_test_split(test_data, test_size=0.5, random_state=SEED, stratify=test_data['category'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7cshgKv4Nij"
      },
      "outputs": [],
      "source": [
        "# Save split datasets\n",
        "train_data.to_csv('train.csv', index=False)\n",
        "valid_data.to_csv('valid.csv', index=False)\n",
        "test_data.to_csv('test.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O84_8P-x4Q28"
      },
      "outputs": [],
      "source": [
        "# Load split datasets\n",
        "train = pd.read_csv('train.csv')\n",
        "valid = pd.read_csv('valid.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using TfIDF vectoriser to extract features from the text data. 'n_grams' is used to ensure that uni-grams and bi-grams are taken into consideration while extracting features. max_features is set to 5000 after trying a varied feature range while training models and 5000 was found to be the optimum feature size for the best F1 score.\n",
        "\n",
        "**F1-Score:** is taken to be our primary metric as this takes into consideration both precision and recall when calculated. This ensures we acknowledge all the misclassifications when trying to improve the model performance."
      ],
      "metadata": {
        "id": "5Tj7gIL6ZPei"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imMkXyt14S9y"
      },
      "outputs": [],
      "source": [
        "# Vectorize text data\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
        "x_train_vectorized = tfidf_vectorizer.fit_transform(train['cleaned_text'])\n",
        "x_valid_vectorized = tfidf_vectorizer.transform(valid['cleaned_text'])\n",
        "y_train = train['category']\n",
        "y_valid = valid['category']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = 1000\n",
        "indices = np.random.choice(range(x_train_vectorized.shape[0]), size=n_samples, replace=False)\n",
        "x_subset = x_train_vectorized[indices]\n",
        "y_subset = y_train.iloc[indices]\n",
        "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
        "tsne_results = tsne.fit_transform(x_subset.toarray())\n",
        "plt.figure(figsize=(12, 8))\n",
        "scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=y_subset, cmap='viridis', alpha=0.6)\n",
        "plt.colorbar(scatter)\n",
        "plt.title('t-SNE visualization of Text Data')\n",
        "plt.xlabel('Component 1')\n",
        "plt.ylabel('Component 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4IeqEKFgrSdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is observed from the above TSNE plot that there is a feature overlap between the two categories. Hence, it is not suggested that we use linear models. Therefor a decision has been made to use **Random Forest Classifier** and **Support Vector Machine**."
      ],
      "metadata": {
        "id": "Q3vuX4itvPh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training **Random Forest Classifier**"
      ],
      "metadata": {
        "id": "zOSdCaFdcuWH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYNuvYLF5nKR"
      },
      "outputs": [],
      "source": [
        "def train_rf(x_train, y_train):\n",
        "    rf = RandomForestClassifier(random_state=SEED)\n",
        "    rf.fit(x_train, y_train)\n",
        "    return rf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txfGFlttRp2H"
      },
      "outputs": [],
      "source": [
        "rf = train_rf(x_train_vectorized, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training **Support Vector Machine**"
      ],
      "metadata": {
        "id": "lBgCjf5ZczNU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otRcYHWo5nmv"
      },
      "outputs": [],
      "source": [
        "def train_svm(x_train, y_train):\n",
        "    svm = SVC(kernel='linear', random_state=SEED)\n",
        "    svm.fit(x_train, y_train)\n",
        "    return svm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we used linear kernel as it was giving the highest F1 Score, but we have to see as we go through the process."
      ],
      "metadata": {
        "id": "0CwECE3Vc-JS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkRIBk7NRtX4"
      },
      "outputs": [],
      "source": [
        "svm = train_svm(x_train_vectorized, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**End-to-end Deep Learning Model**\n",
        "All the parameters below were set to achieve the highest initial F1 score and will be changed if necessary."
      ],
      "metadata": {
        "id": "8JhKfm9idSIr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8MKR3ye5pVg"
      },
      "outputs": [],
      "source": [
        "# Define neural network classifier\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, 256)\n",
        "        self.fc4 = nn.Linear(256, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(self.relu(self.fc1(x)))\n",
        "        x = self.dropout(self.relu(self.fc2(x)))\n",
        "        x = self.dropout(self.relu(self.fc3(x)))\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueIa3Yy05rW-"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "x_train_tensor = torch.tensor(x_train_vectorized.toarray(), dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "x_valid_tensor = torch.tensor(x_valid_vectorized.toarray(), dtype=torch.float32)\n",
        "y_valid_tensor = torch.tensor(y_valid.values, dtype=torch.long)\n",
        "\n",
        "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XPzElnB5s_r"
      },
      "outputs": [],
      "source": [
        "def train_nn(model, criterion, optimizer, train_loader, num_epochs):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        epoch_loss = 0.0\n",
        "        epoch_true = 0\n",
        "        epoch_total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            _, pred = torch.max(outputs, dim=1)\n",
        "            epoch_true += torch.sum(pred == labels).item()\n",
        "            epoch_total += labels.size(0)\n",
        "\n",
        "        train_accuracy = 100 * epoch_true / epoch_total\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        print(f\"Epoch {epoch}/{num_epochs} finished: train_loss = {epoch_loss:.4f}, train_accuracy = {train_accuracy:.2f}%\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_HPzafj5vy5"
      },
      "outputs": [],
      "source": [
        "input_dim = x_train_vectorized.shape[1]\n",
        "categories = 2\n",
        "model = TextClassifier(input_dim, categories).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "trained_model = train_nn(model, criterion, optimizer, train_loader, num_epochs=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4dAPjxt5zAG"
      },
      "outputs": [],
      "source": [
        "# Evaluate models\n",
        "def evaluate_model(model, x_train, y_train, x_valid, y_valid, model_name):\n",
        "    y_train_pred = model.predict(x_train)\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    train_f1 = f1_score(y_train, y_train_pred)\n",
        "    y_valid_pred = model.predict(x_valid)\n",
        "    valid_accuracy = accuracy_score(y_valid, y_valid_pred)\n",
        "    valid_f1 = f1_score(y_valid, y_valid_pred)\n",
        "    print(f\"{model_name} Performance:\")\n",
        "    print(f\"Training Accuracy: {train_accuracy:.4f}, Training F1 Score: {train_f1:.4f}\")\n",
        "    print(f\"Validation Accuracy: {valid_accuracy:.4f}, Validation F1 Score: {valid_f1:.4f}\")\n",
        "    print(\"Classification Report (Validation Set):\")\n",
        "    print(classification_report(y_valid, y_valid_pred))\n",
        "    cnf_matrix = confusion_matrix(y_valid, y_valid_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cnf_matrix)\n",
        "    disp.plot(cmap='Blues_r')\n",
        "    plt.title(f'Confusion Matrix ({model_name} - Validation Set)')\n",
        "    plt.show()\n",
        "    return train_accuracy, train_f1, valid_accuracy, valid_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jM-jXYoi51S7"
      },
      "outputs": [],
      "source": [
        "# Evaluate standard Random Forest\n",
        "rf_train_acc, rf_train_f1, rf_valid_acc, rf_valid_f1 = evaluate_model(rf, x_train_vectorized, y_train, x_valid_vectorized, y_valid, \"Random Forest\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqKiYcXf53F0"
      },
      "outputs": [],
      "source": [
        "# Evaluate SVM\n",
        "svm_train_acc, svm_train_f1, svm_valid_acc, svm_valid_f1 = evaluate_model(svm, x_train_vectorized, y_train, x_valid_vectorized, y_valid, \"SVM\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcXo2dYg698q"
      },
      "outputs": [],
      "source": [
        "# Evaluate neural network\n",
        "def evaluate_nn(model, train_loader, x_valid_tensor, y_valid_tensor):\n",
        "    model.eval()\n",
        "    y_train_true = []\n",
        "    y_train_pred = []\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        y_train_true.extend(labels.cpu().numpy())\n",
        "        y_train_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "    train_accuracy = accuracy_score(y_train_true, y_train_pred)\n",
        "    train_f1 = f1_score(y_train_true, y_train_pred)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        x_valid_tensor = x_valid_tensor.to(device)\n",
        "        y_valid_tensor = y_valid_tensor.to(device)\n",
        "        outputs = model(x_valid_tensor)\n",
        "        _, y_valid_pred = torch.max(outputs, dim=1)\n",
        "\n",
        "    valid_accuracy = accuracy_score(y_valid_tensor.cpu(), y_valid_pred.cpu())\n",
        "    valid_f1 = f1_score(y_valid_tensor.cpu(), y_valid_pred.cpu())\n",
        "\n",
        "    print(f\"Neural Network Performance:\")\n",
        "    print(f\"Training Accuracy: {train_accuracy:.4f}, Training F1 Score: {train_f1:.4f}\")\n",
        "    print(f\"Validation Accuracy: {valid_accuracy:.4f}, Validation F1 Score: {valid_f1:.4f}\")\n",
        "    print(\"Classification Report (Validation Set):\")\n",
        "    print(classification_report(y_valid_tensor.cpu(), y_valid_pred.cpu()))\n",
        "    cnf_matrix = confusion_matrix(y_valid_tensor.cpu(), y_valid_pred.cpu())\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cnf_matrix)\n",
        "    disp.plot(cmap='Blues_r')\n",
        "    plt.title('Confusion Matrix (Neural Network - Validation Set)')\n",
        "    plt.show()\n",
        "\n",
        "    return train_accuracy, train_f1, valid_accuracy, valid_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIiAWu9J54tY"
      },
      "outputs": [],
      "source": [
        "# Evaluate Neural Network\n",
        "nn_train_acc, nn_train_f1, nn_valid_acc, nn_valid_f1 = evaluate_nn(trained_model, train_loader, x_valid_tensor, y_valid_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above evaluation observations, Random Forest Classifier yielded the highest F1 score on validation data, i.e., 79.39%. SVM F1 score = 78.51% and Deep Learning Model = 75.60%"
      ],
      "metadata": {
        "id": "SPcjvn9wdry8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nu-iVXK1_mjb"
      },
      "outputs": [],
      "source": [
        "# Error analysis\n",
        "def error_analysis(model, x_data, y_data, data_text, model_name):\n",
        "    predictions = model.predict(x_data)\n",
        "    errors = np.where(predictions != y_data)[0]\n",
        "    print(f\"Error Analysis for {model_name}: {len(errors)} misclassifications out of {len(y_data)} samples\")\n",
        "\n",
        "    if len(errors) > 5:\n",
        "        sample_errors = np.random.choice(errors, 5, replace=False)\n",
        "    else:\n",
        "        sample_errors = errors\n",
        "\n",
        "    for i in sample_errors:\n",
        "        print(f\"\\nIndex: {i}, Predicted: {predictions[i]}, Actual: {y_data[i]}\")\n",
        "        print(f\"Text: {data_text.iloc[i]}\")\n",
        "    return errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2SlWm_ZAIYl"
      },
      "outputs": [],
      "source": [
        "rf_errors = error_analysis(rf, x_valid_vectorized, y_valid, valid['cleaned_text'], \"Random Forest\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aofDPNVeAibP"
      },
      "outputs": [],
      "source": [
        "svm_errors = error_analysis(svm, x_valid_vectorized, y_valid, valid['cleaned_text'], \"SVM\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyZsO59DAKq7"
      },
      "outputs": [],
      "source": [
        "def nn_error_analysis(model, loader, y_data, data_text, model_name):\n",
        "    model.eval()\n",
        "    predictions, actuals = [], []\n",
        "    text_indices = []\n",
        "    for batch, (inputs, labels) in enumerate(loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        predictions.extend(preds.cpu().numpy())\n",
        "        actuals.extend(labels.cpu().numpy())\n",
        "        text_indices.extend(range(batch * loader.batch_size, batch * loader.batch_size + labels.size(0)))\n",
        "\n",
        "    errors = np.where(np.array(predictions) != np.array(actuals))[0]\n",
        "    print(f\"Error Analysis for {model_name}: {len(errors)} misclassifications out of {len(actuals)} samples\")\n",
        "\n",
        "    if len(errors) > 5:\n",
        "        sample_errors = np.random.choice(errors, 5, replace=False)\n",
        "    else:\n",
        "        sample_errors = errors\n",
        "\n",
        "    for i in sample_errors:\n",
        "        idx = text_indices[i]\n",
        "        print(f\"\\nIndex: {idx}, Predicted: {predictions[i]}, Actual: {actuals[i]}\")\n",
        "        print(f\"Text: {data_text.iloc[idx]}\")\n",
        "    return errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKMCUbrRAOHQ"
      },
      "outputs": [],
      "source": [
        "# Prepare DataLoader for validation set\n",
        "valid_dataset = TensorDataset(x_valid_tensor, y_valid_tensor)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kN9hlXRSj9z"
      },
      "outputs": [],
      "source": [
        "# Perform error analysis on validation set for Neural Network\n",
        "nn_errors_valid = nn_error_analysis(trained_model, valid_loader, y_valid, valid['cleaned_text'], \"Neural Network - Validation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above error analysis it is evident that all the models are misclassifying articles from STYLE more than articles from ENTERTAINMENT. This must be because of the underrepresentation of STYLE category in the original data."
      ],
      "metadata": {
        "id": "_Ciab6ueeMF6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To try and negate this issue of underrepresentation, a couple of approaches were adopted."
      ],
      "metadata": {
        "id": "-nvAErxCejcm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Weighted Samples**"
      ],
      "metadata": {
        "id": "VBBPgLxGesEG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOVCSQTGBhuD"
      },
      "outputs": [],
      "source": [
        "# Refined models with class_weight='balanced'\n",
        "def refined_train_rf(x_train, y_train):\n",
        "    refined_rf = RandomForestClassifier(n_estimators=1000, class_weight='balanced', random_state=SEED)\n",
        "    refined_rf.fit(x_train, y_train)\n",
        "    return refined_rf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zodcccA5SqeU"
      },
      "outputs": [],
      "source": [
        "refined_rf = refined_train_rf(x_train_vectorized, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zAEL3gtCC6X"
      },
      "outputs": [],
      "source": [
        "def refined_train_svm(x_train, y_train):\n",
        "    refined_svm = SVC(kernel='linear', class_weight='balanced', random_state=SEED)\n",
        "    refined_svm.fit(x_train, y_train)\n",
        "    return refined_svm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ca8K8w_kSs94"
      },
      "outputs": [],
      "source": [
        "refined_svm = refined_train_svm(x_train_vectorized, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yQbsHseCH22"
      },
      "outputs": [],
      "source": [
        "# Evaluate refined models\n",
        "refined_rf_train_acc, refined_rf_train_f1, refined_rf_valid_acc, refined_rf_valid_f1 = evaluate_model(refined_rf, x_train_vectorized, y_train, x_valid_vectorized, y_valid, \"Refined Random Forest\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k24F1pqPCl8g"
      },
      "outputs": [],
      "source": [
        "refined_svm_train_acc, refined_svm_train_f1, refined_svm_valid_acc, refined_svm_valid_f1 = evaluate_model(refined_svm, x_train_vectorized, y_train, x_valid_vectorized, y_valid, \"Refined SVM\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ry7ZiAZfCprg"
      },
      "outputs": [],
      "source": [
        "# Neural network with weighted sampler\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "\n",
        "def create_sampler(y_train):\n",
        "    class_counts = np.bincount(y_train)\n",
        "    class_weights = 1. / class_counts\n",
        "    sample_weights = class_weights[y_train]\n",
        "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "    return sampler\n",
        "\n",
        "train_sampler = create_sampler(y_train_tensor.numpy())\n",
        "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False, sampler=train_sampler)\n",
        "\n",
        "refined_model = TextClassifier(input_dim, categories).to(device)\n",
        "optimizer = optim.Adam(refined_model.parameters(), lr=0.001)\n",
        "refined_trained_model = train_nn(refined_model, criterion, optimizer, train_loader, num_epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyzXlbSiEdJu"
      },
      "outputs": [],
      "source": [
        "refined_nn_train_acc, refined_nn_train_f1, refined_nn_valid_acc, refined_nn_valid_f1 = evaluate_nn(refined_trained_model, train_loader, x_valid_tensor, y_valid_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weighted samples did not prove to be useful as the F1 scores were worse compared to before. But the deep learning model's performance has improved noticeably from F1 Score 75.6% to 77.37%"
      ],
      "metadata": {
        "id": "NPI3A0gfe4F5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a combination of **Synthetic Minority Oversampling Technique (SMOTE)** and **Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "18vTy8_NfCYi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsXft8nCNcmi"
      },
      "outputs": [],
      "source": [
        "# SMOTE for imbalanced data\n",
        "smote = SMOTE(random_state=SEED)\n",
        "x_train_smote, y_train_smote = smote.fit_resample(x_train_vectorized, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isfrJU3gNbys"
      },
      "outputs": [],
      "source": [
        "param_grid_rf = {\n",
        "    'n_estimators': [300, 500],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'class_weight': ['balanced']\n",
        "}\n",
        "rf = RandomForestClassifier(random_state=SEED)\n",
        "\n",
        "grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5, verbose=2, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "refined_rf_smote = grid_search_rf.fit(x_train_smote, y_train_smote)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPgTjBw7NhIi"
      },
      "outputs": [],
      "source": [
        "# Evaluate Random Forest with SMOTE\n",
        "refined_rf_smote_train_acc, refined_rf_smote_train_f1, refined_rf_smote_valid_acc, refined_rf_smote_valid_f1 = evaluate_model(refined_rf_smote, x_train_smote, y_train_smote, x_valid_vectorized, y_valid, \"Refined Random Forest with SMOTE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jxjonGBN5X-"
      },
      "outputs": [],
      "source": [
        "param_grid_svm = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'kernel': ['linear', 'rbf', 'sigmoid'],\n",
        "    'class_weight': ['balanced']\n",
        "}\n",
        "\n",
        "# Initialize the SVM classifier\n",
        "svm = SVC(random_state=SEED)\n",
        "\n",
        "# Create the GridSearchCV object for SVM\n",
        "grid_search_svm = GridSearchCV(estimator=svm, param_grid=param_grid_svm, cv=5, verbose=2, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Fit the grid search to the data\n",
        "svm_smote = grid_search_svm.fit(x_train_smote, y_train_smote)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7VXJg4GTPiN"
      },
      "outputs": [],
      "source": [
        "# Evaluate SVM with SMote\n",
        "svm_smote_train_acc, svm_smote_train_f1, svm_smote_valid_acc, svm_smote_valid_f1 = evaluate_model(svm_smote, x_train_smote, y_train_smote, x_valid_vectorized, y_valid, \"SVM with SMOTE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6n4hyHDvN6Qf"
      },
      "outputs": [],
      "source": [
        "# Convert SMOTE data to tensors for Neural Network training\n",
        "x_train_smote_tensor = torch.tensor(x_train_smote.toarray(), dtype=torch.float32)\n",
        "y_train_smote_tensor = torch.tensor(y_train_smote, dtype=torch.long)\n",
        "\n",
        "# DataLoader with SMOTE data\n",
        "train_dataset_smote = TensorDataset(x_train_smote_tensor, y_train_smote_tensor)\n",
        "train_loader_smote = DataLoader(train_dataset_smote, batch_size=64, shuffle=True)\n",
        "\n",
        "nn_smote_model = TextClassifier(input_dim=x_train_smote_tensor.shape[1], output_dim=len(np.unique(y_train_smote))).to(device)\n",
        "optimizer = optim.Adam(nn_smote_model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "trained_nn_smote = train_nn(nn_smote_model, criterion, optimizer, train_loader_smote, num_epochs=10)\n",
        "smote_nn_train_acc, smote_nn_train_f1, smote_nn_valid_acc, smote_nn_valid_f1 = evaluate_nn(trained_nn_smote, train_loader, x_valid_tensor, y_valid_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon performing this, we can see that the F1 score increased for both the Random Forest Classifier and SVM. Now, the SVM turned out to be a better performer than the other two models with an F1 score of 78.36%"
      ],
      "metadata": {
        "id": "AEwjFzOlgUwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saving the models**"
      ],
      "metadata": {
        "id": "DvZoqdpzhZqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_rf_model = grid_search_rf.best_estimator_\n",
        "\n",
        "with open('best_rf_model.pkl', 'wb') as file:\n",
        "    pickle.dump(best_rf_model, file)"
      ],
      "metadata": {
        "id": "mAG1Fkg5_pF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_svm_model = grid_search_svm.best_estimator_\n",
        "\n",
        "with open('best_svm_model.pkl', 'wb') as file:\n",
        "    pickle.dump(best_svm_model, file)"
      ],
      "metadata": {
        "id": "V3J3gBZFCpGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(nn_smote_model.state_dict(), 'nn_smote_model.pth')"
      ],
      "metadata": {
        "id": "PLvPALuO_puI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('best_rf_model.pkl', 'rb') as file:\n",
        "    loaded_rf_model = pickle.load(file)\n",
        "\n",
        "with open('best_svm_model.pkl', 'rb') as file:\n",
        "    loaded_svm_model = pickle.load(file)"
      ],
      "metadata": {
        "id": "5WSTxRuvAtJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Combining the train and validation sets to perform cross validation on models**"
      ],
      "metadata": {
        "id": "7xDBbCH5_wje"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3_BurzfMR1O"
      },
      "outputs": [],
      "source": [
        "# Merge train and validation datasets\n",
        "train_val_combined = pd.concat([train_data, valid_data])\n",
        "\n",
        "# Preprocess text data as previously done\n",
        "X_train_val = tfidf_vectorizer.transform(train_val_combined['cleaned_text'])\n",
        "y_train_val = train_val_combined['category']\n",
        "\n",
        "# Perform cross-validation on the loaded RF and SVM models\n",
        "rf_scores = cross_val_score(loaded_rf_model, X_train_val, y_train_val, cv=5, scoring='accuracy')\n",
        "svm_scores = cross_val_score(loaded_svm_model, X_train_val, y_train_val, cv=5, scoring='accuracy')\n",
        "\n",
        "print(\"Random Forest average cross-validation score:\", np.mean(rf_scores))\n",
        "print(\"SVM average cross-validation score:\", np.mean(svm_scores))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = TextClassifier(input_dim, categories)\n",
        "model.load_state_dict(torch.load('nn_smote_model.pth'))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "X_train_val_vectorized = tfidf_vectorizer.transform(train_val_combined['cleaned_text'])\n",
        "X_train_val_tensor = torch.tensor(X_train_val_vectorized.toarray(), dtype=torch.float32)\n",
        "y_train_val_tensor = torch.tensor(y_train_val.values, dtype=torch.long)\n",
        "full_dataset = TensorDataset(X_train_val_tensor, y_train_val_tensor)\n",
        "full_loader = DataLoader(full_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "hZESCsc-XLi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*KFold Cross Validation for Deep Learning Model*"
      ],
      "metadata": {
        "id": "SXWesK_r_7_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_kfold_nn(model, test_loader):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = correct / total\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    return accuracy, f1"
      ],
      "metadata": {
        "id": "sE4SgPUbH9C-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_validate_nn(model, dataset, k_folds=5):\n",
        "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=SEED)\n",
        "    accuracies = []\n",
        "    f1_scores = []\n",
        "\n",
        "    for train_idx, test_idx in kf.split(dataset):\n",
        "        # Create data loaders for the current fold\n",
        "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
        "        test_loader = DataLoader(dataset, batch_size=64, sampler=test_subsampler)\n",
        "\n",
        "        # Evaluate the model\n",
        "        model.eval()  # Ensure the model is in evaluation mode\n",
        "        accuracy, f1 = evaluate_kfold_nn(model, test_loader)  # Adjust 'evaluate_nn' to handle a single loader\n",
        "        accuracies.append(accuracy)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "    print(f\"Average Accuracy: {np.mean(accuracies)}\")\n",
        "    print(f\"Average F1 Score: {np.mean(f1_scores)}\")\n",
        "\n",
        "# Execute the cross-validation\n",
        "cross_validate_nn(model, full_dataset)"
      ],
      "metadata": {
        "id": "UkI0Y-P4Hc1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After merging the train and valid datasets and performing cross validation, the deep-learning model has a higher Average Cross Validation F1 score of 98.4%."
      ],
      "metadata": {
        "id": "dfOCZzwVhpfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Taking the high CV F1 Score into consideration, the deep learning model was evaluated with the test dataset below."
      ],
      "metadata": {
        "id": "JLmTGWekh_wI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TextClassifier(input_dim, categories)\n",
        "model.load_state_dict(torch.load('nn_smote_model.pth'))\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "9F3q6uLWIQIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_vectorized = tfidf_vectorizer.transform(test_data['cleaned_text'])\n",
        "X_test_tensor = torch.tensor(X_test_vectorized.toarray(), dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(test_data['category'].values, dtype=torch.long)\n",
        "\n",
        "# Create a DataLoader for the test set\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "EJy3lIJaI_Gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_evaluate_nn(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = correct / total\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    return accuracy, f1"
      ],
      "metadata": {
        "id": "IUSitABfI_9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy, f1 = test_evaluate_nn(model, test_loader)"
      ],
      "metadata": {
        "id": "laBPhmn0JGzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We achieved an F1 score of 91.15% without retraining it with the combined dataset of train and valid."
      ],
      "metadata": {
        "id": "L33r_sFiiJYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the model on Combined Dataset and evaluating its performance on Test data**"
      ],
      "metadata": {
        "id": "GEvj7jafiT3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_val_dataset = TensorDataset(X_train_val_tensor, y_train_val_tensor)\n",
        "train_val_loader = DataLoader(train_val_dataset, batch_size=64, shuffle=True)\n",
        "model = TextClassifier(input_dim, categories)\n",
        "model.to(device)\n",
        "model.train()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "train_nn(model, criterion, optimizer, train_val_loader, num_epochs=15)"
      ],
      "metadata": {
        "id": "d3qXtM6HJPua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy, f1 = test_evaluate_nn(model, test_loader)"
      ],
      "metadata": {
        "id": "Wvlh6FWqJ74U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final F1-score also improved from **91.15%** to **91.67%** compared to the model without training on the combined dataset."
      ],
      "metadata": {
        "id": "hd5HqLF6tdmb"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}